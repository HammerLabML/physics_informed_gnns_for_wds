{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper: Physics-Informed Graph Neural Networks for Water Distribution Systems.\n",
    "\n",
    "## Workbook for reproducing the results in Tables 1, 2, 3 and 4.\n",
    "\n",
    "We have included the trained models for all 5 WDS that are placed in the directory 'trained_models'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils.utils import *\n",
    "from dataset_generator import run\n",
    "import pandas as pd\n",
    "import json, argparse, warnings, os\n",
    "import torch\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\"\"\" Specifying the directory for trained models. \"\"\"\n",
    "file_dir = os.getcwd()\n",
    "save_dir = os.path.join(file_dir, \"trained_models\")\n",
    "\n",
    "\"\"\" Creating an output file to log progress. \"\"\"\n",
    "out_f = open(save_dir+\"/output_\"+str(datetime.date.today())+\".txt\", \"a\")\n",
    "\n",
    "\"\"\" \n",
    "        Specifying parameters. \n",
    "        You can specify here, which WDS you want to evaluate \n",
    "        from ['hanoi', 'fossolo', 'pescara', 'l_town', 'zhijiang'].\n",
    "        You can also change the batch size we used for evaluation (2048)\n",
    "        to a smaller values if you are evaluating on a smaller GPU. \n",
    "\"\"\"\n",
    "args_dict = {\n",
    "        \"wdn\": \"l_town\",\n",
    "        \"start_scenario\": 21,\n",
    "        \"end_scenario\": 50,\n",
    "        \"bias\": False,\n",
    "        \"n_days\": 14,\n",
    "        \"batch_size\": 2048,\n",
    "        \"M_n\": 2,\n",
    "        \"out_dim\": 1,\n",
    "        \"M_e\": 2,\n",
    "        \"I\": 5,\n",
    "        \"n_iter\": 10,\n",
    "        \"r_iter\": 10,\n",
    "        \"n_mlp\": 2,\n",
    "        \"M_l\": 128\n",
    "        }\n",
    "args = argparse.Namespace(**args_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Scenarios - Only run if not already generated and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenario_times = []\n",
    "\n",
    "# scenario_times = run(args.wdn, args.start_scenario, args.end_scenario, scenario_times)\n",
    "\n",
    "# wntr_sim_times = {}\n",
    "# for idx, s in enumerate(range(args.start_scenario, args.end_scenario + 1)):\n",
    "#     wntr_sim_times[str(s)] = scenario_times[idx]    \n",
    "\n",
    "# wntr_sim_times_df = pd.DataFrame(wntr_sim_times, index=[0])\n",
    "# wntr_sim_times_df.to_csv(save_dir + \"/wntr_sim_times_\" + args.wdn + '.csv')\n",
    "# print(sum(scenario_times)/60)\n",
    "# display(wntr_sim_times_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading generated scenarios times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wntr_sim_times_df = pd.read_csv(save_dir + \"/wntr_sim_times_\" + args.wdn + '.csv')\n",
    "\n",
    "wntr_sim_time_total = np.round(wntr_sim_times_df.sum(axis=1).values[0], 2)\n",
    "\n",
    "print(\"Time taken to generate scenarios for \" + args.wdn + \": \", wntr_sim_time_total, \"seconds\")\n",
    "\n",
    "display(wntr_sim_times_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_tvt, edge_indices_tvt, edge_attr_tvt, wntr_demands = [], [], [], []\n",
    "\n",
    "\"\"\" Reading scenarios one by one. \"\"\"\n",
    "for s in range(args.start_scenario, args.end_scenario + 1):\n",
    "\n",
    "    \"\"\" Specifying paths for scenario, .inp file and .xlsx file. \"\"\"\n",
    "    scenario_path = os.path.join(os.getcwd(),\"networks\",  args.wdn, \"s\"+str(s))\n",
    "    \"\"\" Since Hanoi has all configuration (*.inp) files available (Vrachimis et al. 2018), we read directly from those.\n",
    "        For the other networks, we read from the .inp files that we created while generating scenarios. These files\n",
    "        have a consistent naming scheme, so we can read those using the WDS name. \"\"\"\n",
    "    if args.wdn == \"hanoi\":\n",
    "        args.inp_file = os.path.join(scenario_path, \"Hanoi_CMH_Scenario-\" + str(s) + \".inp\")\n",
    "    else:\n",
    "        args.inp_file = os.path.join(scenario_path, args.wdn + \".inp\")\n",
    "    args.path_to_data = os.path.join(scenario_path, \"Results-Clean\", \"Measurements_All.xlsx\")\n",
    "\n",
    "    \"\"\" \n",
    "    Loading the dataset from the generated scenario. This returns a \"wdn_graph\" object with following attributes:\n",
    "    X               A (S x N_n x 2) tensor having EPANET/WNTR estimated Heads (h_wntr) and the Original Demands (d_star).\n",
    "    node_coords     Coordinates of nodes that are useful for plotting,\n",
    "    node_indices    Indices of the nodes.     \n",
    "    edge_indices    A (S x 2 x N_e) tensor specifying bidirectional edge connections.  \n",
    "    edge_attr       A (S x N_e x 2) tensor having r and the flows estimated by EPANET/WNTR (q_wntr)\n",
    "    Please note that we load h_wntr and q_wntr primarily for error computations after evaluation and also to get the\n",
    "    reservoir heads (h_star). These are never used in the training of our model. \n",
    "    \"\"\"\n",
    "    wdn_graph, reservoirs = create_graph(args.inp_file, args.path_to_data)\n",
    "\n",
    "    \"\"\" Saving only the number of samples specified. \"\"\"\n",
    "    X_s = wdn_graph.X.clone()\n",
    "    edge_indices_s = wdn_graph.edge_indices.clone()\n",
    "    edge_attr_s = wdn_graph.edge_attr.clone()\n",
    "\n",
    "    \"\"\" Appending these to dataset lists. \"\"\"\n",
    "    X_tvt.append(X_s)\n",
    "    edge_indices_tvt += list(edge_indices_s)\n",
    "    edge_attr_tvt += list(edge_attr_s)\n",
    "\n",
    "    wntr_d_df = pd.read_csv(os.path.join(scenario_path, \"Results-Clean\", \"Measurements_All_Demands.csv\"))\n",
    "    wntr_d_df['Timestamp'] = pd.to_datetime(wntr_d_df['Timestamp'])#, unit='s')\n",
    "    wntr_d_df = wntr_d_df.set_index(\"Timestamp\")\n",
    "    wntr_d = torch.tensor(wntr_d_df.astype(\"float32\").values)\n",
    "    wntr_demands.append(wntr_d)\n",
    "\n",
    "\n",
    "    print('\\nRead Scenario ', str(s))\n",
    "\n",
    "\n",
    "X_tvt = torch.vstack(X_tvt)    \n",
    "d_wntr = torch.vstack(wntr_demands).unsqueeze(2)    \n",
    "\n",
    "wds_tvt = WDN_Graph(X=X_tvt, edge_indices=edge_indices_tvt, edge_attr=edge_attr_tvt)\n",
    "\n",
    "n_nodes = wds_tvt.X.shape[1]\n",
    "n_edges = wds_tvt.edge_indices[0].shape[1]\n",
    "n_samples = wds_tvt.X.shape[0]\n",
    "\n",
    "wds_tvt.X.shape, wds_tvt.edge_indices[0].shape, wds_tvt.edge_attr[0].shape, d_wntr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing WDS Attributes - Table 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "attrib_dict = {}\n",
    "\n",
    "attrib_dict[\"No. of junctions\"] = n_nodes\n",
    "attrib_dict[\"No. of links\"] = n_edges\n",
    "attrib_dict[\"No. of reservoirs\"] = len(reservoirs)\n",
    "\n",
    "G = nx.DiGraph()\n",
    "edge_list = [ (u, v) for u, v in zip(*np.array(wds_tvt.edge_indices[0])) ]\n",
    "G.add_edges_from(edge_list)\n",
    "\n",
    "attrib_dict[\"Diameter\"] = nx.diameter(G)\n",
    "node_degrees = [val for (node, val) in G.degree()]\n",
    "attrib_dict[\"Degree_min\"] = np.round(np.min(node_degrees), 2)\n",
    "attrib_dict[\"Degree_mean\"] = np.round(np.mean(node_degrees), 2)\n",
    "attrib_dict[\"Degree_max\"] = np.round(np.max(node_degrees), 2)\n",
    "\n",
    "attrib_df = pd.DataFrame(attrib_dict, index=[args.wdn])\n",
    "# attrib_df.to_csv(save_dir+'/attributes_'+ args.wdn + '.csv')\n",
    "display(attrib_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_test import *\n",
    "from models.models import *\n",
    "import pickle, os, time\n",
    "\n",
    "\"\"\" Initializing the model and printing the number of parameters. \"\"\"\n",
    "model = PI_GCN( M_n = 2,                    # number of node features (d_star, d_hat).\n",
    "                out_dim = 1,                # out dimension is 1 since only flows are directly estimated.\n",
    "                M_e = 2,                    # number of edge features (q_hat, q_tilde).\n",
    "                M_l = args.M_l,             # specified latent dimension.\n",
    "                I = args.I,                 # number of GCN layers.\n",
    "                num_layers = args.n_mlp,    # number of NN layers used in every MLP.\n",
    "                n_iter = args.n_iter,       # minimum number of iterations.\n",
    "                bias = False                # we do not use any bias.\n",
    "                ).to(device)\n",
    "\n",
    "args.model_path = \"trained_models/\" + args.wdn + \"/model_PI_GCN_\" + args.wdn + \".pt\"\n",
    "model_state = torch.load(args.model_path)\n",
    "model.load_state_dict(model_state[\"model\"])\n",
    "\n",
    "zeta = 0\n",
    "\n",
    "start = time.time()\n",
    "hd_wntr, h_tilde, q_hat, d_hat, test_losses = test(wds_tvt, model, reservoirs, args, save_dir, out_f, zeta=0)\n",
    "eval_time = np.round(time.time() - start, 2)\n",
    "print('Time taken for ', n_samples, ' samples: ', eval_time, ' seconds')\n",
    "\n",
    "e_attr = torch.stack((wds_tvt.edge_attr))\n",
    "e_index = torch.stack((wds_tvt.edge_indices))\n",
    "\n",
    "d_star = wds_tvt.X[:, :, 1:2].clone()\n",
    "d_hat[:, reservoirs, 0] = 0\n",
    "\n",
    "q_wntr = e_attr[:, :, 1:2].clone()\n",
    "h_wntr = hd_wntr[:, :, 0:1].clone()\n",
    "\n",
    "d_star.shape, d_hat.shape, q_wntr.shape, q_hat.shape, h_wntr.shape, h_tilde.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Results dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}\n",
    "not_outlier_parcent = .95\n",
    "\n",
    "results_dict['No. of samples'] = n_samples\n",
    "results_dict['WNTR - Generation time'] = wntr_sim_time_total\n",
    "results_dict['Model - Evaluation time'] = eval_time\n",
    "\n",
    "results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors Computations -  EPANET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import mean_rel_abs_error, get_not_oulier_errors\n",
    "\n",
    "mrae_demands_mean_samples = mean_rel_abs_error(d_star, d_wntr)\n",
    "results_dict['WNTR - Mean MRAE - Demands %'] = np.round(mrae_demands_mean_samples.mean().item() * 100, 8)\n",
    "results_dict['WNTR - Std MRAE - Demands %'] = np.round(mrae_demands_mean_samples.std().item() * 100, 8)\n",
    "mrae_demands_mean_samples_no = get_not_oulier_errors(mrae_demands_mean_samples, not_outlier_parcent)\n",
    "results_dict['WNTR - NO-Mean MRAE - Demands %'] = np.round(mrae_demands_mean_samples_no.mean().item() * 100, 8)\n",
    "results_dict['WNTR - NO-Std MRAE - Demands %'] = np.round(mrae_demands_mean_samples_no.std().item() * 100, 8)\n",
    "\n",
    "results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors Computations -  Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mrae_demands_mean_samples = mean_rel_abs_error(d_star, d_hat)\n",
    "results_dict['Model - Mean MRAE - Demands %'] = np.round(mrae_demands_mean_samples.mean().item() * 100, 8)\n",
    "results_dict['Model - Std MRAE - Demands %'] = np.round(mrae_demands_mean_samples.std().item() * 100, 8)\n",
    "mrae_demands_mean_samples_no = get_not_oulier_errors(mrae_demands_mean_samples, not_outlier_parcent)\n",
    "results_dict['Model - NO-Mean MRAE - Demands %'] = np.round(mrae_demands_mean_samples_no.mean().item() * 100, 8)\n",
    "results_dict['Model - NO-Std MRAE - Demands %'] = np.round(mrae_demands_mean_samples_no.std().item() * 100, 8)\n",
    "\n",
    "mrae_flows_mean_samples = mean_rel_abs_error(q_wntr, q_hat)\n",
    "results_dict['Model - Mean MRAE - Flows %'] = np.round(mrae_flows_mean_samples.mean().item() * 100, 8)\n",
    "results_dict['Model - Std MRAE - Flows %'] = np.round(mrae_flows_mean_samples.std().item() * 100, 8)\n",
    "mrae_flows_mean_samples_no = get_not_oulier_errors(mrae_flows_mean_samples, not_outlier_parcent)\n",
    "results_dict['Model - NO-Mean MRAE - Flows %'] = np.round(mrae_flows_mean_samples_no.mean().item() * 100, 8)\n",
    "results_dict['Model - NO-Std MRAE - Flows %'] = np.round(mrae_flows_mean_samples_no.std().item() * 100, 8)\n",
    "\n",
    "mrae_heads_mean_samples = mean_rel_abs_error(h_wntr, h_tilde)\n",
    "results_dict['Model - Mean MRAE - Heads %'] = np.round(mrae_heads_mean_samples.mean().item() * 100, 8)\n",
    "results_dict['Model - Std MRAE - Heads %'] = np.round(mrae_heads_mean_samples.std().item() * 100, 8)\n",
    "mrae_heads_mean_samples_no = get_not_oulier_errors(mrae_heads_mean_samples, not_outlier_parcent)\n",
    "results_dict['Model - NO-Mean MRAE - Heads %'] = np.round(mrae_heads_mean_samples_no.mean().item() * 100, 8)\n",
    "results_dict['Model - NO-Std MRAE - Heads %'] = np.round(mrae_heads_mean_samples_no.std().item() * 100, 8)\n",
    "\n",
    "results_df = pd.DataFrame(results_dict, index=[args.wdn]).transpose()\n",
    "# results_df.to_csv(save_dir+'/results_'+ args.wdn + '.csv')\n",
    "\n",
    "display(results_df)\n",
    "results_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "waterfutures",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
